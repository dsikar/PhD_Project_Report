\section*{Cheat Sheet: Histogram Binning Rules}

\begin{description}

  \item[\textbf{Sturges' Rule} \cite{sturges1926}]  
  Designed for small samples, based on a binomial-to-normal approximation.  
  Number of bins:
  \[
    k = 1 + \log_{2}(n),
  \]
  where $n$ is the sample size.  
  \emph{Tends to undersmooth for large $n$, but simple and widely implemented.}

  \item[\textbf{Scott's Rule} \cite{scott1979}]  
  Minimises the asymptotic mean integrated squared error (MISE) assuming a normal distribution.  
  Bin width:
  \[
    h = \frac{3.5 \, \hat{\sigma}}{n^{1/3}},
  \]
  where $\hat{\sigma}$ is the sample standard deviation.  
  Number of bins is then $\text{range}(x) / h$.  
  \emph{Good for approximately normal data, but sensitive to outliers.}

  \item[\textbf{Freedman--Diaconis Rule} \cite{freedman1981}]  
  Robust alternative to Scott’s rule, replacing $\hat{\sigma}$ with the interquartile range (IQR).  
  Bin width:
  \[
    h = \frac{2 \, \mathrm{IQR}(x)}{n^{1/3}}.
  \]
  Number of bins is again $\text{range}(x) / h$.  
  \emph{More robust for heavy-tailed or skewed data.}

  \item[\textbf{Knuth’s Bayesian Method} \cite{knuth2006}]  
  Treats the histogram as a multinomial likelihood model with a uniform prior over bins.  
  The optimal number of bins $k$ is chosen to maximise the posterior:
  \[
    p(k \mid \text{data}) \propto p(\text{data} \mid k) \, p(k).
  \]
  \emph{Automatic, data-driven, adapts to sample size and distribution.}

  \item[\textbf{Birg{\'e}--Rozenholc Penalised Likelihood} \cite{birge2006}]  
  Poses bin selection as a model selection problem with a penalised likelihood criterion.  
  The number of bins $k$ is chosen by minimising:
  \[
    -\log L(k) + \text{penalty}(k),
  \]
  where $L(k)$ is the likelihood of the histogram model with $k$ bins.  
  \emph{Provides theoretical guarantees and adapts well to different data shapes.}

\end{description}


Here’s what the two rules would suggest for your dataset ($n = 28{,}000$, steering range $-4.5^\circ$ to $+4.5^\circ$, i.e. total range $\approx 9^\circ$):

* **Sturges’ Rule**:

  $$
  k = 1 + \log_2(28000) \;\approx\; 15.8 \;\;\Rightarrow\;\; \text{about 16 bins.}
  $$

* **Scott’s Rule**:
  Using an estimated $\sigma \approx 1.5^\circ$ (from the assumption that $6\sigma \approx$ data range),

  $$
  h = \frac{3.5 \hat{\sigma}}{n^{1/3}} \;\approx\; 0.173^\circ,
  $$

  which yields

  $$
  k \approx \frac{9}{0.173} \;\approx\; 52 \text{ bins.}
  $$

---

So, compared with your empirical choices (3, 5, 15 bins):

* **Sturges** is remarkably close to your largest tested case (15).
* **Scott** would recommend a much finer discretisation (\~52 bins), which would almost certainly have left many classes underpopulated given the skewed distribution of steering data.

Would you like me to also compute the **Freedman–Diaconis** rule for this dataset, assuming we can approximate or guess the IQR of steering angles (e.g. concentrated around 0°)? That could give you a third comparison point.

I computed the Freedman–Diaconis (FD) result using the same rough normal approximation you allowed before ($\hat\sigma \approx \text{range}/6 = 1.5^\circ$, hence $\mathrm{IQR}\approx 1.349\hat\sigma$). Results:

* Assumed $\hat\sigma \approx 1.5^\circ$.
* $\mathrm{IQR}\approx 2.0235^\circ$.
* FD bin width:

  $$
  h_{\mathrm{FD}} = \frac{2\,\mathrm{IQR}}{n^{1/3}} \approx 0.1333^\circ.
  $$
* FD number of bins:

  $$
  k_{\mathrm{FD}} \approx \frac{9^\circ}{0.1333^\circ} \approx 67.5 \;\Rightarrow\; \text{about 67 bins.}
  $$

Summary (for $n=28{,}000$, range $=9^\circ$, approximate $\hat\sigma=1.5^\circ$):

* Sturges: $\approx 16$ bins.
* Scott: $\approx 52$ bins.
* Freedman–Diaconis: $\approx 67$ bins.

Caveats & recommendations:

* These numbers assume a roughly normal spread across the $\pm4.5^\circ$ range. In practice steering angles are typically highly peaked around 0°, so the effective $\sigma$ and IQR will be much smaller — which would increase the number of bins suggested by Scott and FD even further (because the formulas scale with the estimated spread).
* Because the steering-angle distribution is concentrated near zero, many of these fine-bin recommendations would produce very sparse bins in the tails; that explains why your empirical 3–5 bin choices performed better.
* If you want exact FD/Scott results for your dataset, compute $\hat\sigma$ and $\mathrm{IQR}$ directly from your steering labels and re-evaluate the formulas — I can generate that code/snippet if you want.

Would you like a ready-to-paste LaTeX table summarising these three rule outputs (using the computed numbers) for your methods section?

