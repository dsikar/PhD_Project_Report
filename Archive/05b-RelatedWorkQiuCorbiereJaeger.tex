%%%%%%%%%%%%%%%%%%
%% RELATED WORK %%
%%%%%%%%%%%%%%%%%%

\chapter{Related work}
\label{related_work}


% Hi Daniel,

% It is worth checking out and consider adding the papers below to your list of references in case you haven't already. You can do this after the submission deadline too.

%  Qiu, X., Miikkulainen, R.: Detecting misclassification errors in neural networks with a gaussian process model. In: AAAI (2022)
% https://arxiv.org/pdf/2010.02065
% https://notebooklm.google.com/notebook/19178385-96f4-47bd-b9af-5a6c84af1e98

%  Corbière, C., Thome, N., Bar-Hen, A., Cord, M., Pérez, P.: Addressing failure prediction by 66 learning model confidence. In: NeurIPS (2019)
% https://notebooklm.google.com/notebook/56423006-0e6f-4034-8e16-2af21ccd1ef4

% Jaeger, P.F., Lüth, C.T., Klein, L., Bungert, T.J.: A call to reflect on evaluation practices for 403 failure detection in image classification. arXiv preprint arXiv:2211.15259 (2022)

% The first two focus on misclassification and confidence. The latter is also related to OOD learning. All seem relevant to your work.

% Best,
% Artur

% START 
%  Qiu, X., Miikkulainen, R.: Detecting misclassification errors in neural networks with a gaussian process model. In: AAAI (2022)
% https://arxiv.org/pdf/2010.02065
% https://notebooklm.google.com/notebook/19178385-96f4-47bd-b9af-5a6c84af1e98


\documentclass{article}
\usepackage{amsmath} % For mathematical notation
\usepackage{amssymb} % For mathematical symbols
\begin{document}

The paper presents Residual-based Error Detection (RED), a framework designed to detect misclassification errors in neural network classifiers \cite{QiuMiikkulainen2022}. A central innovation of RED is its ability to not only provide a detection score but also to report the uncertainty associated with that score \cite{QiuMiikkulainen2022, Mandelbaum2017}. This is a crucial improvement over existing confidence metrics, which are often unreliable or misleading \cite{Provost1998}.

RED's methodology is built upon the RIO (Residual prediction with Input/Output kernel) method \cite{Qiu2020}, which quantifies point-prediction uncertainty in regression models. To adapt RIO for classification error detection, the following key modifications are implemented:
\begin{enumerate}
    \item The original RIO's output kernel, designed for scalar outputs, is extended to handle multi-output vector data, such as the softmax probabilities generated by a neural network classifier. This new variant is referred to as mRIO (multi-output RIO) \cite{QiuMiikkulainen2022}.
    \item Instead of directly learning ground-truths, RED learns to predict whether the original classification is correct or incorrect. A target detection score $c_i$ is assigned to each training data point using the Kronecker delta: $c_i = \delta_{y_i,\hat{y}_i}$ (where $\delta_{y_i,\hat{y}_i} = 1$ if $y_i = \hat{y}_i$, otherwise $\delta_{y_i,\hat{y}_i} = 0$) \cite{QiuMiikkulainen2022}.
    \item The framework then calculates residuals $r_i = c_i - \hat{c}_i$ between this target score $c_i$ and the original maximum class probability $\hat{c}_i = \max(\sigma_i)$, which is the highest softmax output. The mRIO model is trained to predict these residuals \cite{QiuMiikkulainen2022}.
\end{enumerate}
In practical deployment, RED provides a Gaussian distribution for the estimated detection score $\hat{c}'_* \sim N(\hat{c}_* + \bar{\hat{r}}_*, \text{var}(\hat{r}_*))$ \cite{QiuMiikkulainen2022}. The mean, $\hat{c}_* + \bar{\hat{r}}_*$, serves as the detection score for potential misclassifications, while the variance, $\text{var}(\hat{r}_*)$, explicitly represents the uncertainty of this score \cite{QiuMiikkulainen2022}. This explicit reporting of uncertainty is a distinguishing characteristic of RED.

\begin{thebibliography}{9}

\bibitem{Mandelbaum2017}
Mandelbaum, A.; and Weinshall, D. 2017.
Distance-based Confidence Score for Neural Network Classifiers.
\textit{ArXiv preprint arXiv:1709.09844}.

\bibitem{Provost1998}
Provost, F. J.; Fawcett, T.; and Kohavi, R. 1998.
The Case against Accuracy Estimation for Comparing Induction Algorithms.
In \textit{Proceedings of the Fifteenth International Conference on Machine Learning}, 445--453. San Francisco, CA, USA.

\bibitem{Qiu2020}
Qiu, X.; Meyerson, E.; and Miikkulainen, R. 2020.
Quantifying Point-Prediction Uncertainty in Neural Networks via Residual Estimation with an I/O Kernel.
In \textit{International Conference on Learning Representations}.

\bibitem{QiuMiikkulainen2022}
Qiu, X.; and Miikkulainen, R. 2022.
Detecting Misclassification Errors in Neural Networks with a Gaussian Process Model.
\textit{ArXiv preprint arXiv:2010.02065}.

\end{thebibliography}

\end{document}

% END
%  Qiu, X., Miikkulainen, R.: Detecting misclassification errors in neural networks with a gaussian process model. In: AAAI (2022)
% https://arxiv.org/pdf/2010.02065
% https://notebooklm.google.com/notebook/19178385-96f4-47bd-b9af-5a6c84af1e98


% START
%  Corbière, C., Thome, N., Bar-Hen, A., Cord, M., Pérez, P.: Addressing failure prediction by 66 learning model confidence. In: NeurIPS (2019)

\documentclass{article}
\usepackage{amsmath}
\usepackage{hyperref}

\begin{document}

The paper addresses predicting failures in deep neural networks, a crucial aspect for their real-world deployment, especially in high-stakes applications like autonomous driving \cite{1, 2, 3}. A common confidence measure, Maximum Class Probability (MCP), given by the softmax output of the predicted class, is often unreliable because it can yield high confidence even for incorrect predictions \cite{3, 5, 11}. To overcome this, the authors propose True Class Probability (TCP) as a novel confidence criterion \cite{1, 6}. TCP is defined as the probability of the model with respect to the true class, $P(Y = y^*|w,x)$ \cite{12}. This measure inherently associates lower values with misclassifications and higher values with correct predictions, significantly improving the separation between them \cite{5, 12}. Since the true class is unknown at test time, they introduce ConfidNet, a neural network designed to learn this TCP criterion during training \cite{14, 15}. ConfidNet builds on features from a pre-trained classification model and is trained using an $\mathcal{l}_2$ loss function \cite{15, 16, 17}. This approach consistently outperforms methods like Maximum Class Probability (MCP) \cite{17}, Trust Score \cite{20}, and Monte-Carlo Dropout (MCDropout) \cite{10} across various datasets, demonstrating its effectiveness \cite{1, 27, 28}.

\begin{thebibliography}{99}
\bibitem{1} Corbière, Charles, Nicolas Thome, Avner Bar-Hen, Matthieu Cord, and Patrick Pérez. "Addressing Failure Prediction by Learning Model Confidence." \textit{arXiv preprint arXiv:1910.04851} (2019).
\bibitem{2} Amodei, Dario, Chris Olah, Jacob Steinhardt, Paul F. Christiano, John Schulman, and Dan Mané. "Concrete problems in AI safety." \textit{arXiv preprint arXiv:1606.06565} (2016).
\bibitem{3} Corbière, Charles, Nicolas Thome, Avner Bar-Hen, Matthieu Cord, and Patrick Pérez. "Addressing Failure Prediction by Learning Model Confidence." \textit{arXiv preprint arXiv:1910.04851} (2019).
\bibitem{5} Corbière, Charles, Nicolas Thome, Avner Bar-Hen, Matthieu Cord, and Patrick Pérez. "Addressing Failure Prediction by Learning Model Confidence." \textit{arXiv preprint arXiv:1910.04851} (2019).
\bibitem{6} Corbière, Charles, Nicolas Thome, Avner Bar-Hen, Matthieu Cord, and Patrick Pérez. "Addressing Failure Prediction by Learning Model Confidence." \textit{arXiv preprint arXiv:1910.04851} (2019).
\bibitem{10} Gal, Yarin, and Zoubin Ghahramani. "Dropout as a bayesian approximation: Representing model uncertainty in deep learning." In \textit{International conference on machine learning}, pp. 1050-1059. 2016.
\bibitem{11} Corbière, Charles, Nicolas Thome, Avner Bar-Hen, Matthieu Cord, and Patrick Pérez. "Addressing Failure Prediction by Learning Model Confidence." \textit{arXiv preprint arXiv:1910.04851} (2019).
\bibitem{12} Corbière, Charles, Nicolas Thome, Avner Bar-Hen, Matthieu Cord, and Patrick Pérez. "Addressing Failure Prediction by Learning Model Confidence." \textit{arXiv preprint arXiv:1910.04851} (2019).
\bibitem{14} Corbière, Charles, Nicolas Thome, Avner Bar-Hen, Matthieu Cord, and Patrick Pérez. "Addressing Failure Prediction by Learning Model Confidence." \textit{arXiv preprint arXiv:1910.04851} (2019).
\bibitem{15} Corbière, Charles, Nicolas Thome, Avner Bar-Hen, Matthieu Cord, and Patrick Pérez. "Addressing Failure Prediction by Learning Model Confidence." \textit{arXiv preprint arXiv:1910.04851} (2019).
\bibitem{16} Corbière, Charles, Nicolas Thome, Avner Bar-Hen, Matthieu Cord, and Patrick Pérez. "Addressing Failure Prediction by Learning Model Confidence." \textit{arXiv preprint arXiv:1910.04851} (2019).
\bibitem{17} Hendrycks, Dan, and Kevin Gimpel. "A baseline for detecting misclassified and out-of-distribution examples in neural networks." In \textit{International Conference on Learning Representations}. 2017.
\bibitem{20} Jiang, Heinrich, Been Kim, Melody Guan, and Maya Gupta. "To trust or not to trust a classifier." In \textit{Advances in Neural Information Processing Systems}, pp. 5530-5540. 2018.
\bibitem{27} Corbière, Charles, Nicolas Thome, Avner Bar-Hen, Matthieu Cord, and Patrick Pérez. "Addressing Failure Prediction by Learning Model Confidence." \textit{arXiv preprint arXiv:1910.04851} (2019).
\bibitem{28} Corbière, Charles, Nicolas Thome, Avner Bar-Hen, Matthieu Cord, and Patrick Pérez. "Addressing Failure Prediction by Learning Model Confidence." \textit{arXiv preprint arXiv:1910.04851} (2019).
\end{thebibliography}

\end{document}


% END
%  Corbière, C., Thome, N., Bar-Hen, A., Cord, M., Pérez, P.: Addressing failure prediction by 66 learning model confidence. In: NeurIPS (2019)

% START 
% Jaeger, P.F., Lüth, C.T., Klein, L., Bungert, T.J.: A call to reflect on evaluation practices for
% 403 failure detection in image classification. arXiv preprint arXiv:2211.15259 (2022)
%https://notebooklm.google.com/notebook/cc73cd24-7d82-4145-bcb8-6f67688e1b79

\documentclass{article}
\usepackage{ragged2e}
\usepackage{amsmath} % For mathematical notation

\begin{document}

\RaggedRight % Align text to the left

The paper primarily focuses on proposing a unified evaluation protocol for failure detection in image classification, addressing inconsistencies and shortcomings in existing methods. It highlights three main pitfalls in current evaluation practices: heterogeneous task definitions, a limited scope of relevant failure sources, and a discrepancy between stated purpose and actual evaluation.

The core of the proposed methodology is a unified task formulation where the classification output can be filtered if a Confidence Scoring Function (CSF) `g(x)` falls below a threshold $\tau$. A CSF is expected to assign high confidence to correct predictions and low confidence to incorrect ones, based on the binary failure label defined as $y_f(x,w, y) = I(y_{cl} \neq \hat{y}_m(x,w))$. The paper advocates for the Area under the Risk-Coverage-Curve (AURC) as the primary evaluation metric. AURC intrinsically assesses both the classifier's accuracy and the CSF's ranking performance, providing a single score that directly reflects the overarching goal of preventing silent failures. This approach fulfills the requirements for comprehensive evaluation, considering the CSF's impact on the classifier, encouraging diverse failure source assessment, and using direct failure information rather than surrogate labels. The empirical study evaluates various CSFs, including Maximum Softmax Response (MSR), Monte Carlo Dropout (MCD) variants, ConfidNet, DeepGamblers, and Mahalanobis Distance.

\bibliographystyle{plain}
\begin{thebibliography}{99}
\bibitem{Source1} Excerpts from "2211.15259v2.pdf"
\bibitem{Source2} Excerpts from "2211.15259v2.pdf"
\bibitem{Source3} Excerpts from "2211.15259v2.pdf"
\bibitem{Source4} Excerpts from "2211.15259v2.pdf"
\bibitem{Source7} Excerpts from "2211.15259v2.pdf"
\bibitem{Source9} Excerpts from "2211.15259v2.pdf"
\bibitem{Source10} Excerpts from "2211.15259v2.pdf"
\bibitem{Source11} Excerpts from "2211.15259v2.pdf"
\bibitem{Source18} Excerpts from "2211.15259v2.pdf"
\end{thebibliography}

\end{document}


% END
% Jaeger, P.F., Lüth, C.T., Klein, L., Bungert, T.J.: A call to reflect on evaluation practices for
% 403 failure detection in image classification. arXiv preprint arXiv:2211.15259 (2022)
